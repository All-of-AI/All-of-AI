## 深度学习9 深度强化学习

**强化学习(reinforcement learning, RL)**，也叫增强学习，是指一类**从(与环境)交互中不断学习**的问题以及解决这类问题的方法。强化学习问题可以描述为一个智能体从与环境的交互中不断学习以完成特定目标(比如**取得最大奖励值**)。和深度学习类似，强化学习中的关键问题是**贡献度分配问题**，每一个动作并不能直接得到监督信息，需要通过整个模型的最终监督信息(奖励)得到，并且有一定的延时性。 

### 9.1 强化学习问题

#### 9.1.1 典型例子

(1) **多臂赌博机问题**：给定$K$个赌博机，拉动每个赌博机的拉杆，赌博机会**按照一个事先设定的概率掉出一块钱或不掉钱**。**每个赌博机掉钱的概率不一样**。多臂赌博机问题是指，给定有限的机会次数$T$，**如何玩这些赌博机才能使得期望累积收益最大化**。 

(2) **悬崖行走问题**：在一个网格世界中，每个格子表示一个状态。如下图所示的一个网格世界，每个状态为$(i, j)$，其中格子$(2*,* 1)$到$(6*,* 1)$是悬崖。有一个醉汉，从左下角的开始位置$S$，走到右下角的目标位置$E$。如果走到悬崖，醉汉会跌落悬崖并死去。醉汉可以选择行走的路线，即在每个状态时，选择行走的方向：上下左右。**动作空间**$A=\{\uparrow, \downarrow, \leftarrow, \rightarrow\}$。但每走一步，都**有一定的概率滑落到周围其他的格子**。目标是如何安全地到达目标位置。

<img src="images/image-20201028192723607.png" style="zoom:45%;" />

#### 9.1.2 强化学习的定义

在强化学习中，有两个可以进行交互的对象”：智能体和环境。

**智能体(agent)**可以感知**外界环境的状态(state)和反馈的奖励(reward)**，并进行学习和决策。智能体的**决策**功能是指**根据外界环境的状态**来做出不同的**动作(action)**，而**学习**功能是指**根据外界环境的奖励来调整策略**。

**环境(environment)**是智能体外部的事物，并受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励。

强化学习的基本要素包括：

(1) **状态**$s$是对环境的描述，可以是离散的或连续的，其**状态空间**为$\mathcal S$；

(2) **动作**$a$是对智能体行为的描述，可以是离散的或连续的，其**动作空间**为$\mathcal A$；

(3) **策略**$\pi(a|s)$是智能体根据环境状态$s$来决定下一步动作$a$的函数，通常可以分为**确定性策略和随机性策略**两种。

确定性策略(deterministic policy)是从状态空间到动作空间的**映射函数**$\pi: \mathcal S \rightarrow \mathcal A$，随机性策略(stochastic policy)表示在给定环境状态时，智能体选择某个动作的**概率分布**：$\pi(a|s)=p(a|s)$。**通常，随机性策略更为常用**。

(4) **状态转移概率**$p(s'|s,a)$是在智能体根据当前状态$s$做出动作$a$之后，环境在下一个时刻转变为状态$s'$的概率；

(5) **即时奖励**$r(s,a,s')$是一个标量函数，即智能体根据当前状态$s$做出动作$a$之后，环境会反馈给智能体一个奖励，这个奖励也技能长和下一个时刻的状态$s'$有关。

#### 9.1.3 马尔可夫决策过程

为了简单起见，我们将智能体与环境的交互看作是离散的时间序列。下图给出了智能体与环境的交互。智能体从感知到的初始环境$s_0$开始，然后决定做一个相应的动作$a_0$，环境相应地发生改变到新的状态$s_1$，并反馈给智能体一个即时奖励$r_1$，然后智能体又根据状态$s_1$做一个动作$a_1$，环境相应改变为$s_2$，并反馈奖励$r_2$。这样的交互可以一直进行下去：$s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, \cdots, s_{t-1}, r_{t-1}, a_{t-1}, s_{t}, r_{t}, \cdots$，其中$r_t=r(s_{t-1}, a_{t-1}, s_t)$是第$t$时刻的奖励。

<img src="images/image-20201028210854347.png" style="zoom:45%;" />

智能体与环境的交互过程可以看做是一个马尔可夫决策过程。

**马尔可夫过程(Markov process)**是具有马尔可夫性质的随机变量序列$s_{0}, s_{1}, \cdots, s_{t} \in \mathcal{S}$，其下一时刻的状态$s_{t+1}$值取决于当前状态$s_t$，即
$$
p(s_{t+1} | s_{t}, \cdots, s_{0})=p(s_{t+1} | s_{t})
$$

其中$p(s_{t+1}|s_t)$称为状态转移概率，$\sum_{s_{t+1} \in \mathcal S}p(s_{t+1}|s_t)=1$。

**马尔可夫决策过程(Markov decision process, MDP)**在马尔可夫过程中加入一个额外的变量：动作$a$，即下一个时刻的状态$s_{t+1}$和当前时刻的状态$s_t$以及动作$a_t$相关：
$$
p(s_{t+1} | s_{t}, a_{t}, \cdots, s_{0}, a_{0})=p(s_{t+1} | s_{t}, a_{t})
$$
其中$p(s_{t+1} | s_{t}, a_{t})$为状态转移概率。给定策略$\pi(a|s)$，马尔可夫决策过程的一个**轨迹**
$$
\tau=s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, \cdots, s_{T-1}, a_{T-1}, s_{T}, r_{T}
$$
的概率为：
$$
\begin{aligned}
p(\tau) &=p(s_{0}, a_{0}, s_{1}, a_{1}, \cdots) \\
&=p(s_{0}) \prod_{t=0}^{T-1} \pi(a_{t} | s_{t}) p(s_{t+1} \mid s_{t}, a_{t})
\end{aligned}
$$
下图给出了马尔可夫决策过程的图模型表示：

<img src="images/image-20201028213258928.png" style="zoom:50%;" />



### 9.2 



### 9.3 



### 参考资料

[1] 邱锡鹏. 神经网络与深度学习. 2019.

