## 机器学习12 降维

### 12.1 主成分分析

**主成分分析(principal component analysis, PCA)**是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。如下图所示的二维数据，如果将这些数据投影到一维空间，选择数据方差最大的方向进行投影(蓝轴)，才能最大化数据的差异性，保留更多的原始数据信息。

<img src="images/image-20200528202055368.png" style="zoom:40%;" />

假设有一组$d$维样本$\boldsymbol x^{(n)} \in \mathbb R^d, 1 \leqslant n \leqslant N$，我们希望将其投影到一维空间中，投影向量为$\boldsymbol w \in \mathbb R^d$。不失一般性，限制$\boldsymbol w$的模为1，即$\boldsymbol w^\text T \boldsymbol w=1$。每个样本点$\boldsymbol x^{(n)}$投影之后的表示为$z^{(n)}=\boldsymbol w^\text T \boldsymbol x^{(n)}$。

用矩阵$X=\left[\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \cdots, \boldsymbol{x}^{(N)}\right]$表示输入样本，$\bar{\boldsymbol{x}}=\frac{1}{N} \sum_{n=1}^{N} \boldsymbol{x}^{(n)}$为原始样本点的中心店，所有样本**投影后的方差**为：
$$
\begin{aligned}
\sigma(X ; \boldsymbol{w}) &=\frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}^{(n)}-\boldsymbol{w}^{\mathrm{T}} \bar{\boldsymbol{x}})^{2} \\
&=\frac{1}{N}(\boldsymbol{w}^{\mathrm{T}} X-\boldsymbol{w}^{\mathrm{T}} \bar{X})(\boldsymbol{w}^{\mathrm{T}} X-\boldsymbol{w}^{\mathrm{T}} \bar{X})^{\mathrm{T}} \\
&=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{S} \boldsymbol{w}
\end{aligned}
$$
其中，$\bar X=\bar{\boldsymbol x} \boldsymbol 1_d^\text T$为$d$列$\bar{\boldsymbol x}$组成的矩阵，$S=\frac{1}{N}(X-\bar{X})(X-\bar{X})^{\mathrm{T}}$是**原始样本的协方差矩阵**。

最大化投影方差$\sigma(X;\boldsymbol w)$，并满足$\boldsymbol w^\text T \boldsymbol w=1$，**利用拉格朗日方法转换为无约束优化问题**：
$$
\max _{\boldsymbol{w}} \boldsymbol{w}^{\mathrm{T}} S \boldsymbol{w}+\lambda(1-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{w})
$$
其中$\lambda$为拉格朗日乘子。对上式求导并令导数为0，可得$S \boldsymbol{w}=\lambda \boldsymbol{w}$。

从上式可知，$\boldsymbol w$是协方差矩阵$S$的特征向量，$\lambda$为特征值。同时，$\sigma(X ; w)=\boldsymbol{w}^{\mathrm{T}} S \boldsymbol{w}=\boldsymbol{w}^{\mathrm{T}} \lambda \boldsymbol{w}=\lambda$。

$\lambda$也是投影后样本的方差。因此，主成分分析可以转换成一个矩阵特征值分解问题，投影向量$\boldsymbol w$为矩阵$S$的**最大特征值对应的特征向量**。

如果要通过投影矩阵$W \in R^{d \times d^{\prime}}$将样本投到$d^\prime$维空间，投影矩阵满足$W^\text T W=\boldsymbol I$，只需要将$S$的**特征值从大到小排列**，保留前$d^\prime$个特征值，**其对应的特征向量即是最优的投影矩阵**。

主成分分析是一种无监督学习方法，可以作为监督学习的数据预处理方法，用来**去除噪声并减少特征之间的相关性**，但是它**不能保证投影后数据的类别可分性更好**。提高两类可分性的方法一般为监督学习方法，比如**线性判别分析(linear discriminant analysis, LDA)**。

### 12.2 流形学习



### 参考资料

[1] 李航. 统计学习方法. 北京: 清华大学出版社, 2019.

[2] 周志华. 机器学习. 北京: 清华大学出版社, 2016.

[3] 邱锡鹏. 神经网络与深度学习. 2019.